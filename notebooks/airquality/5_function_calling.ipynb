{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "00f44e17-ed2d-47e7-887d-c07490a50f83",
      "metadata": {
        "id": "00f44e17-ed2d-47e7-887d-c07490a50f83"
      },
      "source": [
        "## <span style='color:#ff5f27'> üìù Colab Users - Uncomment & Run the following 2 Cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e353e1e-ace8-4ee5-9bef-86a9155e764b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e353e1e-ace8-4ee5-9bef-86a9155e764b",
        "outputId": "957ec0cc-f800-4505-e4c3-e0352ea99e25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'id2223-lab1' already exists and is not an empty directory.\n",
            "[Errno 2] No such file or directory: 'mlfs-book'\n",
            "/content/mlfs-book/mlfs-book\n",
            "Requirement already satisfied: uv in /usr/local/lib/python3.12/dist-packages (0.9.8)\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m228 packages\u001b[0m \u001b[2min 115ms\u001b[0m\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m228 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.31)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.35)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.41)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.29)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.11.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.104.2 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (23.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (0.11.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.104.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Google Colab environment\n",
            "Added the following directory to the PYTHONPATH: /content/mlfs-book/mlfs-book\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def is_google_colab() -> bool:\n",
        "    if \"google.colab\" in str(get_ipython()):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def clone_repository() -> None:\n",
        "    !rm -rf id2223-lab1 #Remove any old copies if we have run the notebook before\n",
        "    !git clone https://github.com/Isabell257/id2223-lab1\n",
        "    %cd id2223-lab1\n",
        "\n",
        "def install_dependencies() -> None:\n",
        "    !pip install --upgrade uv\n",
        "    !uv pip install --all-extras --system --requirement pyproject.toml\n",
        "    !pip install langchain langchain-community langchain-openai #Adding because it does not work\n",
        "\n",
        "    #Adding because version too old otherwise\n",
        "    !pip install -U bitsandbytes \n",
        "    !pip install -U transformers accelerate safetensors \n",
        "\n",
        "if is_google_colab():\n",
        "    clone_repository()\n",
        "    install_dependencies()\n",
        "    root_dir = str(Path().absolute())\n",
        "    print(\"Google Colab environment\")\n",
        "else:\n",
        "    root_dir = Path().absolute()\n",
        "    # Strip ~/notebooks/ccfraud from PYTHON_PATH if notebook started in one of these subdirectories\n",
        "    if root_dir.parts[-1:] == ('airquality',):\n",
        "        root_dir = Path(*root_dir.parts[:-1])\n",
        "    if root_dir.parts[-1:] == ('notebooks',):\n",
        "        root_dir = Path(*root_dir.parts[:-1])\n",
        "    root_dir = str(root_dir)\n",
        "    print(\"Local environment\")\n",
        "\n",
        "# Add the root directory to the `PYTHONPATH` to use the `recsys` Python module from the notebook.\n",
        "if root_dir not in sys.path:\n",
        "    sys.path.append(root_dir)\n",
        "print(f\"Added the following directory to the PYTHONPATH: {root_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2f3f016",
      "metadata": {
        "id": "a2f3f016"
      },
      "source": [
        "## <span style='color:#ff5f27'> üìù Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "721ae546",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "721ae546",
        "outputId": "2f51e05d-397c-4e6b-e4f2-1b1003cc5fa8"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'functions'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1992759059.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhopsworks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from mlfs.airquality.llm_chain import (\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mload_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mget_llm_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mlfs-book/mlfs/airquality/llm_chain.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_engineering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_context_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msafetensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'functions'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBRegressor\n",
        "import hopsworks\n",
        "from openai import OpenAI\n",
        "from mlfs.airquality.llm_chain import (\n",
        "    load_model,\n",
        "    get_llm_chain,\n",
        "    generate_response,\n",
        "    generate_response_openai,\n",
        ")\n",
        "import pandas as pd\n",
        "import os\n",
        "import warnings\n",
        "from mlfs import config\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b062cc0",
      "metadata": {
        "id": "3b062cc0"
      },
      "source": [
        "## <span style=\"color:#ff5f27;\"> üîÆ Connect to Hopsworks Feature Store </span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6340e8e",
      "metadata": {
        "id": "b6340e8e"
      },
      "outputs": [],
      "source": [
        "#settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")\n",
        "project = hopsworks.login()\n",
        "fs = project.get_feature_store()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6f2f191",
      "metadata": {
        "id": "b6f2f191"
      },
      "outputs": [],
      "source": [
        "# Get_or_create the 'air_quality_fv' feature view\n",
        "feature_view = fs.get_feature_view(\n",
        "    name='air_quality_fv',\n",
        "    version=1\n",
        ")\n",
        "\n",
        "# Initialize batch scoring\n",
        "feature_view.init_batch_scoring(1)\n",
        "\n",
        "weather_fg = fs.get_feature_group(\n",
        "    name='weather',\n",
        "    version=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8002765b",
      "metadata": {
        "id": "8002765b"
      },
      "source": [
        "## <span style=\"color:#ff5f27;\">ü™ù Retrieve AirQuality Model from Model Registry</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02695f9e",
      "metadata": {
        "id": "02695f9e"
      },
      "outputs": [],
      "source": [
        "# Retrieve the model registry\n",
        "mr = project.get_model_registry()\n",
        "\n",
        "# Retrieve the 'air_quality_xgboost_model' from the model registry\n",
        "retrieved_model = mr.get_model(\n",
        "    name=\"air_quality_xgboost_model\",\n",
        "    version=1,\n",
        ")\n",
        "\n",
        "# Download the saved model artifacts  to a local directory\n",
        "saved_model_dir = retrieved_model.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8930caa5",
      "metadata": {
        "id": "8930caa5"
      },
      "outputs": [],
      "source": [
        "# Loading the XGBoost regressor model and label encoder from the saved model directory\n",
        "# model_air_quality = joblib.load(saved_model_dir + \"/xgboost_regressor.pkl\")\n",
        "model_air_quality = XGBRegressor()\n",
        "\n",
        "model_air_quality.load_model(saved_model_dir + \"/model.json\")\n",
        "\n",
        "# Displaying the retrieved XGBoost regressor model\n",
        "model_air_quality"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd30142d",
      "metadata": {
        "id": "fd30142d"
      },
      "source": [
        "## <span style='color:#ff5f27'>‚¨áÔ∏è LLM Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a911a86c",
      "metadata": {
        "id": "a911a86c"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# Load the LLM and its corresponding tokenizer.\n",
        "model_llm, tokenizer = load_model(model_id=\"imiraoui/OpenHermes-2.5-Mistral-7B-sharded\")\n",
        "\n",
        "duration = time.time() - start_time\n",
        "print(f\"The code execution took {duration} seconds.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e329285",
      "metadata": {
        "id": "0e329285"
      },
      "source": [
        "## <span style='color:#ff5f27'>‚õìÔ∏è LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8caf5ffa",
      "metadata": {
        "id": "8caf5ffa"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "# Create and configure a language model chain.\n",
        "llm_chain = get_llm_chain(\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        ")\n",
        "\n",
        "duration = time.time() - start_time\n",
        "print(f\"The code execution took {duration} seconds.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a2ded5c",
      "metadata": {
        "id": "4a2ded5c"
      },
      "source": [
        "## <span style='color:#ff5f27'>üß¨ Domain-specific Evaluation Harness\n",
        "\n",
        "**Systematic evaluations** that can run automatically in CI/CD pipelines are key to evaluating models/RAG.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58181b2b",
      "metadata": {
        "id": "58181b2b"
      },
      "outputs": [],
      "source": [
        "QUESTION7 = \"Hi!\"\n",
        "\n",
        "response7 = generate_response(\n",
        "    QUESTION7,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec32e56",
      "metadata": {
        "id": "4ec32e56"
      },
      "outputs": [],
      "source": [
        "QUESTION = \"Who are you?\"\n",
        "\n",
        "response = generate_response(\n",
        "    QUESTION,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d58ca1f",
      "metadata": {
        "id": "4d58ca1f"
      },
      "outputs": [],
      "source": [
        "QUESTION1 = \"What was the average air quality from 2024-01-10 till 2024-01-14?\"\n",
        "\n",
        "response1 = generate_response(\n",
        "    QUESTION1,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41d01dbc",
      "metadata": {
        "id": "41d01dbc"
      },
      "outputs": [],
      "source": [
        "QUESTION11 = \"When and what was the air quality like last week?\"\n",
        "\n",
        "response11 = generate_response(\n",
        "    QUESTION11,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb2d1a38",
      "metadata": {
        "id": "eb2d1a38"
      },
      "outputs": [],
      "source": [
        "QUESTION12 = \"When and what was the minimum air quality from 2024-01-10 till 2024-01-14?\"\n",
        "\n",
        "response12 = generate_response(\n",
        "    QUESTION12,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "659bad46",
      "metadata": {
        "id": "659bad46"
      },
      "outputs": [],
      "source": [
        "QUESTION2a = \"What was the air quality like last week?\"\n",
        "\n",
        "response2 = generate_response(\n",
        "    QUESTION2a,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c35e6bef",
      "metadata": {
        "id": "c35e6bef"
      },
      "outputs": [],
      "source": [
        "QUESTION2 = \"What was the air quality like yesterday?\"\n",
        "\n",
        "response2 = generate_response(\n",
        "    QUESTION2,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed349483",
      "metadata": {
        "id": "ed349483"
      },
      "outputs": [],
      "source": [
        "QUESTION3 = \"What will the air quality be like next Tuesday?\"\n",
        "\n",
        "response3 = generate_response(\n",
        "    QUESTION3,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e6825c6",
      "metadata": {
        "id": "5e6825c6"
      },
      "outputs": [],
      "source": [
        "QUESTION4 = \"What will the air quality be like the day after tomorrow?\"\n",
        "\n",
        "response4 = generate_response(\n",
        "    QUESTION4,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09ac0709",
      "metadata": {
        "id": "09ac0709"
      },
      "outputs": [],
      "source": [
        "QUESTION5 = \"What will the air quality be like this Sunday?\"\n",
        "\n",
        "response5 = generate_response(\n",
        "    QUESTION5,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee271416",
      "metadata": {
        "id": "ee271416"
      },
      "outputs": [],
      "source": [
        "QUESTION7 = \"What will the air quality be like for the rest of the week?\"\n",
        "\n",
        "response7 = generate_response(\n",
        "    QUESTION7,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aeeb4ec",
      "metadata": {
        "id": "9aeeb4ec"
      },
      "outputs": [],
      "source": [
        "QUESTION = \"Will the air quality be safe or not for the next week?\"\n",
        "\n",
        "response = generate_response(\n",
        "    QUESTION7,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe8b4e60",
      "metadata": {
        "id": "fe8b4e60"
      },
      "outputs": [],
      "source": [
        "QUESTION = \"Is tomorrow's air quality level dangerous?\"\n",
        "\n",
        "response = generate_response(\n",
        "    QUESTION,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fb26726",
      "metadata": {
        "id": "4fb26726"
      },
      "outputs": [],
      "source": [
        "QUESTION = \"Can you please explain different PM2_5 air quality levels?\"\n",
        "\n",
        "response = generate_response(\n",
        "    QUESTION,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a463f7",
      "metadata": {
        "id": "c2a463f7"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09fb77d2",
      "metadata": {
        "id": "09fb77d2"
      },
      "outputs": [],
      "source": [
        "# !pip install openai --quiet\n",
        "# !pip install gradio==3.40.1 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f4aebe3",
      "metadata": {
        "id": "9f4aebe3"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "from functions.llm_chain import load_model, get_llm_chain, generate_response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a442d20d",
      "metadata": {
        "id": "a442d20d"
      },
      "outputs": [],
      "source": [
        "# Initialize the ASR pipeline\n",
        "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
        "\n",
        "def transcribe(audio):\n",
        "    sr, y = audio\n",
        "    y = y.astype(np.float32)\n",
        "    if y.ndim > 1 and y.shape[1] > 1:\n",
        "        y = np.mean(y, axis=1)\n",
        "    y /= np.max(np.abs(y))\n",
        "    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n",
        "\n",
        "def generate_query_response(user_query, method, openai_api_key=None):\n",
        "    if method == 'Hermes LLM':\n",
        "        response = generate_response(\n",
        "            user_query,\n",
        "            feature_view,\n",
        "            weather_fg,\n",
        "            model_air_quality,\n",
        "            model_llm,\n",
        "            tokenizer,\n",
        "            llm_chain,\n",
        "            verbose=False,\n",
        "        )\n",
        "        return response\n",
        "\n",
        "    elif method == 'OpenAI API' and openai_api_key:\n",
        "        client = OpenAI(\n",
        "            api_key=openai_api_key\n",
        "        )\n",
        "\n",
        "        response = generate_response_openai(\n",
        "            user_query,\n",
        "            feature_view,\n",
        "            weather_fg,\n",
        "            model_air_quality,\n",
        "            client=client,\n",
        "            verbose=True,\n",
        "        )\n",
        "        return response\n",
        "\n",
        "    else:\n",
        "        return \"Invalid method or missing API key.\"\n",
        "\n",
        "def handle_input(text_input=None, audio_input=None, method='Hermes LLM', openai_api_key=\"\"):\n",
        "    if audio_input is not None:\n",
        "        user_query = transcribe(audio_input)\n",
        "    else:\n",
        "        user_query = text_input\n",
        "\n",
        "    # Check if OpenAI API key is required but not provided\n",
        "    if method == 'OpenAI API' and not openai_api_key.strip():\n",
        "        return \"OpenAI API key is required for this method.\"\n",
        "\n",
        "    if user_query:\n",
        "        return generate_query_response(user_query, method, openai_api_key)\n",
        "    else:\n",
        "        return \"Please provide input either via text or voice.\"\n",
        "\n",
        "\n",
        "# Setting up the Gradio Interface\n",
        "iface = gr.Interface(\n",
        "    fn=handle_input,\n",
        "    inputs=[\n",
        "        gr.Textbox(placeholder=\"Type here or use voice input...\"),\n",
        "        gr.Audio(),\n",
        "        gr.Radio([\"Hermes LLM\", \"OpenAI API\"], label=\"Choose the response generation method\"),\n",
        "        gr.Textbox(label=\"Enter your OpenAI API key (only if you selected OpenAI API):\", type=\"password\")  # Removed `optional=True`\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"üå§Ô∏è AirQuality AI Assistant üí¨\",\n",
        "    description=\"Ask your questions about air quality or use your voice to interact. Select the response generation method and provide an OpenAI API key if necessary.\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4afa7c6a",
      "metadata": {
        "id": "4afa7c6a"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
