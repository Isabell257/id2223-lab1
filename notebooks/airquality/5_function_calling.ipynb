{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "00f44e17-ed2d-47e7-887d-c07490a50f83",
      "metadata": {
        "id": "00f44e17-ed2d-47e7-887d-c07490a50f83"
      },
      "source": [
        "## <span style='color:#ff5f27'> üìù Colab Users - Uncomment & Run the following 2 Cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0e353e1e-ace8-4ee5-9bef-86a9155e764b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e353e1e-ace8-4ee5-9bef-86a9155e764b",
        "outputId": "90ad9866-afdb-4452-80e9-e9694c3dff75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mlfs-book'...\n",
            "remote: Enumerating objects: 1857, done.\u001b[K\n",
            "remote: Total 1857 (delta 0), reused 0 (delta 0), pack-reused 1857 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1857/1857), 17.23 MiB | 17.15 MiB/s, done.\n",
            "Resolving deltas: 100% (945/945), done.\n",
            "/content/mlfs-book\n",
            "Collecting uv\n",
            "  Downloading uv-0.9.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading uv-0.9.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uv\n",
            "Successfully installed uv-0.9.8\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m228 packages\u001b[0m \u001b[2min 6.36s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m68 packages\u001b[0m \u001b[2min 7m 08s\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m18 packages\u001b[0m \u001b[2min 438ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m68 packages\u001b[0m \u001b[2min 294ms\u001b[0m\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1maltair\u001b[0m\u001b[2m==5.5.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1maltair\u001b[0m\u001b[2m==4.2.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1masync-lru\u001b[0m\u001b[2m==2.0.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mavro\u001b[0m\u001b[2m==1.11.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mboto3\u001b[0m\u001b[2m==1.40.70\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbotocore\u001b[0m\u001b[2m==1.40.70\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcattrs\u001b[0m\u001b[2m==25.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcolorama\u001b[0m\u001b[2m==0.4.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcomm\u001b[0m\u001b[2m==0.2.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mconfluent-kafka\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdnspython\u001b[0m\u001b[2m==2.8.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1memail-validator\u001b[0m\u001b[2m==2.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfaker\u001b[0m\u001b[2m==37.12.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfastavro\u001b[0m\u001b[2m==1.11.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfeldera\u001b[0m\u001b[2m==0.41.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfurl\u001b[0m\u001b[2m==2.1.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mgreat-expectations\u001b[0m\u001b[2m==0.18.12\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhopsworks\u001b[0m\u001b[2m==4.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhopsworks-aiomysql\u001b[0m\u001b[2m==0.2.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.24.7\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==8.7.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mimportlib-metadata\u001b[0m\u001b[2m==6.11.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==6.17.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mipykernel\u001b[0m\u001b[2m==7.1.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjavaobj-py3\u001b[0m\u001b[2m==0.4.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjedi\u001b[0m\u001b[2m==0.19.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjh2\u001b[0m\u001b[2m==5.0.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjmespath\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjson5\u001b[0m\u001b[2m==0.12.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==7.4.9\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjupyter-client\u001b[0m\u001b[2m==8.6.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjupyter-lsp\u001b[0m\u001b[2m==2.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjupyterlab\u001b[0m\u001b[2m==4.4.10\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mjupyterlab-server\u001b[0m\u001b[2m==2.28.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmakefun\u001b[0m\u001b[2m==1.16.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmarshmallow\u001b[0m\u001b[2m==3.26.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.10.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.8.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mmock\u001b[0m\u001b[2m==5.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mniquests\u001b[0m\u001b[2m==3.15.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnotebook\u001b[0m\u001b[2m==6.5.7\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnotebook\u001b[0m\u001b[2m==7.4.7\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.0.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==1.26.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopenmeteo-requests\u001b[0m\u001b[2m==1.7.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopenmeteo-sdk\u001b[0m\u001b[2m==1.23.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mopensearch-py\u001b[0m\u001b[2m==2.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1morderedmultidict\u001b[0m\u001b[2m==1.0.1\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==25.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==23.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==10.4.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mpolars\u001b[0m\u001b[2m==1.25.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpolars\u001b[0m\u001b[2m==0.20.31\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpretty-errors\u001b[0m\u001b[2m==1.2.25\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==5.29.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==4.25.8\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpydeck\u001b[0m\u001b[2m==0.9.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyhumps\u001b[0m\u001b[2m==1.6.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpyjks\u001b[0m\u001b[2m==20.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mpymysql\u001b[0m\u001b[2m==1.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mqh3\u001b[0m\u001b[2m==1.5.6\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mrequests-cache\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mretry-requests\u001b[0m\u001b[2m==2.0.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mretrying\u001b[0m\u001b[2m==1.4.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mruamel-yaml\u001b[0m\u001b[2m==0.17.17\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1ms3transfer\u001b[0m\u001b[2m==0.14.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.6.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.2.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1msentence-transformers\u001b[0m\u001b[2m==5.1.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msentence-transformers\u001b[0m\u001b[2m==2.2.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1msqlalchemy\u001b[0m\u001b[2m==2.0.44\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1msqlalchemy\u001b[0m\u001b[2m==2.0.29\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mstreamlit\u001b[0m\u001b[2m==1.28.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.4\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.48.3\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtwofish\u001b[0m\u001b[2m==0.3.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1murl-normalize\u001b[0m\u001b[2m==2.2.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1murllib3-future\u001b[0m\u001b[2m==2.14.906\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mvalidators\u001b[0m\u001b[2m==0.35.0\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mwassima\u001b[0m\u001b[2m==2.0.2\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mxgboost\u001b[0m\u001b[2m==3.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mxgboost\u001b[0m\u001b[2m==2.0.3\u001b[0m\n",
            "Google Colab environment\n",
            "Added the following directory to the PYTHONPATH: /content/mlfs-book\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "def is_google_colab() -> bool:\n",
        "    if \"google.colab\" in str(get_ipython()):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def clone_repository() -> None:\n",
        "    !git clone https://github.com/featurestorebook/mlfs-book.git\n",
        "    %cd mlfs-book\n",
        "\n",
        "def install_dependencies() -> None:\n",
        "    !pip install --upgrade uv\n",
        "    !uv pip install --all-extras --system --requirement pyproject.toml\n",
        "\n",
        "if is_google_colab():\n",
        "    clone_repository()\n",
        "    install_dependencies()\n",
        "    root_dir = str(Path().absolute())\n",
        "    print(\"Google Colab environment\")\n",
        "else:\n",
        "    root_dir = Path().absolute()\n",
        "    # Strip ~/notebooks/ccfraud from PYTHON_PATH if notebook started in one of these subdirectories\n",
        "    if root_dir.parts[-1:] == ('airquality',):\n",
        "        root_dir = Path(*root_dir.parts[:-1])\n",
        "    if root_dir.parts[-1:] == ('notebooks',):\n",
        "        root_dir = Path(*root_dir.parts[:-1])\n",
        "    root_dir = str(root_dir)\n",
        "    print(\"Local environment\")\n",
        "\n",
        "# Add the root directory to the `PYTHONPATH` to use the `recsys` Python module from the notebook.\n",
        "if root_dir not in sys.path:\n",
        "    sys.path.append(root_dir)\n",
        "print(f\"Added the following directory to the PYTHONPATH: {root_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2f3f016",
      "metadata": {
        "id": "a2f3f016"
      },
      "source": [
        "## <span style='color:#ff5f27'> üìù Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "721ae546",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "721ae546",
        "outputId": "e991d491-8079-4e91-bfdc-dcffdba23567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "\n",
            "WARNING:py.warnings:DeprecationWarning: numpy.core is deprecated and has been renamed to numpy._core. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.multiarray.\n",
            "\n",
            "WARNING:py.warnings:DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_community'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1992759059.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhopsworks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mopenai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m from mlfs.airquality.llm_chain import (\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mload_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mget_llm_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mlfs-book/mlfs/airquality/llm_chain.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFacePipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLLMChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/llms/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mllms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;31m# If not in interactive env, raise warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from xgboost import XGBRegressor\n",
        "import hopsworks\n",
        "from openai import OpenAI\n",
        "from mlfs.airquality.llm_chain import (\n",
        "    load_model,\n",
        "    get_llm_chain,\n",
        "    generate_response,\n",
        "    generate_response_openai,\n",
        ")\n",
        "import pandas as pd\n",
        "import os\n",
        "import warnings\n",
        "from mlfs import config\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b062cc0",
      "metadata": {
        "id": "3b062cc0"
      },
      "source": [
        "## <span style=\"color:#ff5f27;\"> üîÆ Connect to Hopsworks Feature Store </span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6340e8e",
      "metadata": {
        "id": "b6340e8e"
      },
      "outputs": [],
      "source": [
        "settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")\n",
        "project = hopsworks.login()\n",
        "fs = project.get_feature_store()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6f2f191",
      "metadata": {
        "id": "b6f2f191"
      },
      "outputs": [],
      "source": [
        "# Get_or_create the 'air_quality_fv' feature view\n",
        "feature_view = fs.get_feature_view(\n",
        "    name='air_quality_fv',\n",
        "    version=1\n",
        ")\n",
        "\n",
        "# Initialize batch scoring\n",
        "feature_view.init_batch_scoring(1)\n",
        "\n",
        "weather_fg = fs.get_feature_group(\n",
        "    name='weather',\n",
        "    version=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8002765b",
      "metadata": {
        "id": "8002765b"
      },
      "source": [
        "## <span style=\"color:#ff5f27;\">ü™ù Retrieve AirQuality Model from Model Registry</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02695f9e",
      "metadata": {
        "id": "02695f9e"
      },
      "outputs": [],
      "source": [
        "# Retrieve the model registry\n",
        "mr = project.get_model_registry()\n",
        "\n",
        "# Retrieve the 'air_quality_xgboost_model' from the model registry\n",
        "retrieved_model = mr.get_model(\n",
        "    name=\"air_quality_xgboost_model\",\n",
        "    version=1,\n",
        ")\n",
        "\n",
        "# Download the saved model artifacts  to a local directory\n",
        "saved_model_dir = retrieved_model.download()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8930caa5",
      "metadata": {
        "id": "8930caa5"
      },
      "outputs": [],
      "source": [
        "# Loading the XGBoost regressor model and label encoder from the saved model directory\n",
        "# model_air_quality = joblib.load(saved_model_dir + \"/xgboost_regressor.pkl\")\n",
        "model_air_quality = XGBRegressor()\n",
        "\n",
        "model_air_quality.load_model(saved_model_dir + \"/model.json\")\n",
        "\n",
        "# Displaying the retrieved XGBoost regressor model\n",
        "model_air_quality"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd30142d",
      "metadata": {
        "id": "fd30142d"
      },
      "source": [
        "## <span style='color:#ff5f27'>‚¨áÔ∏è LLM Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a911a86c",
      "metadata": {
        "id": "a911a86c"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# Load the LLM and its corresponding tokenizer.\n",
        "model_llm, tokenizer = load_model(model_id=\"imiraoui/OpenHermes-2.5-Mistral-7B-sharded\")\n",
        "\n",
        "duration = time.time() - start_time\n",
        "print(f\"The code execution took {duration} seconds.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e329285",
      "metadata": {
        "id": "0e329285"
      },
      "source": [
        "## <span style='color:#ff5f27'>‚õìÔ∏è LangChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8caf5ffa",
      "metadata": {
        "id": "8caf5ffa"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "# Create and configure a language model chain.\n",
        "llm_chain = get_llm_chain(\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        ")\n",
        "\n",
        "duration = time.time() - start_time\n",
        "print(f\"The code execution took {duration} seconds.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a2ded5c",
      "metadata": {
        "id": "4a2ded5c"
      },
      "source": [
        "## <span style='color:#ff5f27'>üß¨ Domain-specific Evaluation Harness\n",
        "\n",
        "**Systematic evaluations** that can run automatically in CI/CD pipelines are key to evaluating models/RAG.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58181b2b",
      "metadata": {
        "id": "58181b2b"
      },
      "outputs": [],
      "source": [
        "QUESTION7 = \"Hi!\"\n",
        "\n",
        "response7 = generate_response(\n",
        "    QUESTION7,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ec32e56",
      "metadata": {
        "id": "4ec32e56"
      },
      "outputs": [],
      "source": [
        "QUESTION = \"Who are you?\"\n",
        "\n",
        "response = generate_response(\n",
        "    QUESTION,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d58ca1f",
      "metadata": {
        "id": "4d58ca1f"
      },
      "outputs": [],
      "source": [
        "QUESTION1 = \"What was the average air quality from 2024-01-10 till 2024-01-14?\"\n",
        "\n",
        "response1 = generate_response(\n",
        "    QUESTION1,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41d01dbc",
      "metadata": {
        "id": "41d01dbc"
      },
      "outputs": [],
      "source": [
        "QUESTION11 = \"When and what was the air quality like last week?\"\n",
        "\n",
        "response11 = generate_response(\n",
        "    QUESTION11,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response11)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb2d1a38",
      "metadata": {
        "id": "eb2d1a38"
      },
      "outputs": [],
      "source": [
        "QUESTION12 = \"When and what was the minimum air quality from 2024-01-10 till 2024-01-14?\"\n",
        "\n",
        "response12 = generate_response(\n",
        "    QUESTION12,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "659bad46",
      "metadata": {
        "id": "659bad46"
      },
      "outputs": [],
      "source": [
        "QUESTION2a = \"What was the air quality like last week?\"\n",
        "\n",
        "response2 = generate_response(\n",
        "    QUESTION2a,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c35e6bef",
      "metadata": {
        "id": "c35e6bef"
      },
      "outputs": [],
      "source": [
        "QUESTION2 = \"What was the air quality like yesterday?\"\n",
        "\n",
        "response2 = generate_response(\n",
        "    QUESTION2,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed349483",
      "metadata": {
        "id": "ed349483"
      },
      "outputs": [],
      "source": [
        "QUESTION3 = \"What will the air quality be like next Tuesday?\"\n",
        "\n",
        "response3 = generate_response(\n",
        "    QUESTION3,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e6825c6",
      "metadata": {
        "id": "5e6825c6"
      },
      "outputs": [],
      "source": [
        "QUESTION4 = \"What will the air quality be like the day after tomorrow?\"\n",
        "\n",
        "response4 = generate_response(\n",
        "    QUESTION4,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09ac0709",
      "metadata": {
        "id": "09ac0709"
      },
      "outputs": [],
      "source": [
        "QUESTION5 = \"What will the air quality be like this Sunday?\"\n",
        "\n",
        "response5 = generate_response(\n",
        "    QUESTION5,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee271416",
      "metadata": {
        "id": "ee271416"
      },
      "outputs": [],
      "source": [
        "QUESTION7 = \"What will the air quality be like for the rest of the week?\"\n",
        "\n",
        "response7 = generate_response(\n",
        "    QUESTION7,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aeeb4ec",
      "metadata": {
        "id": "9aeeb4ec"
      },
      "outputs": [],
      "source": [
        "QUESTION = \"Will the air quality be safe or not for the next week?\"\n",
        "\n",
        "response = generate_response(\n",
        "    QUESTION7,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe8b4e60",
      "metadata": {
        "id": "fe8b4e60"
      },
      "outputs": [],
      "source": [
        "QUESTION = \"Is tomorrow's air quality level dangerous?\"\n",
        "\n",
        "response = generate_response(\n",
        "    QUESTION,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fb26726",
      "metadata": {
        "id": "4fb26726"
      },
      "outputs": [],
      "source": [
        "QUESTION = \"Can you please explain different PM2_5 air quality levels?\"\n",
        "\n",
        "response = generate_response(\n",
        "    QUESTION,\n",
        "    feature_view,\n",
        "    weather_fg,\n",
        "    model_air_quality,\n",
        "    model_llm,\n",
        "    tokenizer,\n",
        "    llm_chain,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a463f7",
      "metadata": {
        "id": "c2a463f7"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09fb77d2",
      "metadata": {
        "id": "09fb77d2"
      },
      "outputs": [],
      "source": [
        "# !pip install openai --quiet\n",
        "# !pip install gradio==3.40.1 --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f4aebe3",
      "metadata": {
        "id": "9f4aebe3"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "from functions.llm_chain import load_model, get_llm_chain, generate_response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a442d20d",
      "metadata": {
        "id": "a442d20d"
      },
      "outputs": [],
      "source": [
        "# Initialize the ASR pipeline\n",
        "transcriber = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\")\n",
        "\n",
        "def transcribe(audio):\n",
        "    sr, y = audio\n",
        "    y = y.astype(np.float32)\n",
        "    if y.ndim > 1 and y.shape[1] > 1:\n",
        "        y = np.mean(y, axis=1)\n",
        "    y /= np.max(np.abs(y))\n",
        "    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n",
        "\n",
        "def generate_query_response(user_query, method, openai_api_key=None):\n",
        "    if method == 'Hermes LLM':\n",
        "        response = generate_response(\n",
        "            user_query,\n",
        "            feature_view,\n",
        "            weather_fg,\n",
        "            model_air_quality,\n",
        "            model_llm,\n",
        "            tokenizer,\n",
        "            llm_chain,\n",
        "            verbose=False,\n",
        "        )\n",
        "        return response\n",
        "\n",
        "    elif method == 'OpenAI API' and openai_api_key:\n",
        "        client = OpenAI(\n",
        "            api_key=openai_api_key\n",
        "        )\n",
        "\n",
        "        response = generate_response_openai(\n",
        "            user_query,\n",
        "            feature_view,\n",
        "            weather_fg,\n",
        "            model_air_quality,\n",
        "            client=client,\n",
        "            verbose=True,\n",
        "        )\n",
        "        return response\n",
        "\n",
        "    else:\n",
        "        return \"Invalid method or missing API key.\"\n",
        "\n",
        "def handle_input(text_input=None, audio_input=None, method='Hermes LLM', openai_api_key=\"\"):\n",
        "    if audio_input is not None:\n",
        "        user_query = transcribe(audio_input)\n",
        "    else:\n",
        "        user_query = text_input\n",
        "\n",
        "    # Check if OpenAI API key is required but not provided\n",
        "    if method == 'OpenAI API' and not openai_api_key.strip():\n",
        "        return \"OpenAI API key is required for this method.\"\n",
        "\n",
        "    if user_query:\n",
        "        return generate_query_response(user_query, method, openai_api_key)\n",
        "    else:\n",
        "        return \"Please provide input either via text or voice.\"\n",
        "\n",
        "\n",
        "# Setting up the Gradio Interface\n",
        "iface = gr.Interface(\n",
        "    fn=handle_input,\n",
        "    inputs=[\n",
        "        gr.Textbox(placeholder=\"Type here or use voice input...\"),\n",
        "        gr.Audio(),\n",
        "        gr.Radio([\"Hermes LLM\", \"OpenAI API\"], label=\"Choose the response generation method\"),\n",
        "        gr.Textbox(label=\"Enter your OpenAI API key (only if you selected OpenAI API):\", type=\"password\")  # Removed `optional=True`\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"üå§Ô∏è AirQuality AI Assistant üí¨\",\n",
        "    description=\"Ask your questions about air quality or use your voice to interact. Select the response generation method and provide an OpenAI API key if necessary.\"\n",
        ")\n",
        "\n",
        "iface.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4afa7c6a",
      "metadata": {
        "id": "4afa7c6a"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}