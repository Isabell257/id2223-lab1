{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4447764c-218b-441a-ab97-4df4062960d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local environment\n",
      "Added the following directory to the PYTHONPATH: /Users/isabell/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def is_google_colab() -> bool:\n",
    "    if \"google.colab\" in str(get_ipython()):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def clone_repository() -> None:\n",
    "    !git clone https://github.com/featurestorebook/mlfs-book.git\n",
    "    %cd mlfs-book\n",
    "\n",
    "def install_dependencies() -> None:\n",
    "    !pip install --upgrade uv\n",
    "    !uv pip install --all-extras --system --requirement pyproject.toml\n",
    "\n",
    "if is_google_colab():\n",
    "    clone_repository()\n",
    "    install_dependencies()\n",
    "    root_dir = str(Path().absolute())\n",
    "    print(\"Google Colab environment\")\n",
    "else:\n",
    "    root_dir = Path().absolute()\n",
    "    # Strip ~/notebooks/ccfraud from PYTHON_PATH if notebook started in one of these subdirectories\n",
    "    if root_dir.parts[-1:] == ('airquality',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    if root_dir.parts[-1:] == ('notebooks',):\n",
    "        root_dir = Path(*root_dir.parts[:-1])\n",
    "    root_dir = str(root_dir) \n",
    "    print(\"Local environment\")\n",
    "\n",
    "# Add the root directory to the `PYTHONPATH` to use the `recsys` Python module from the notebook.\n",
    "if root_dir not in sys.path:\n",
    "    sys.path.append(root_dir)\n",
    "print(f\"Added the following directory to the PYTHONPATH: {root_dir}\")\n",
    "    \n",
    "# Set the environment variables from the file <root_dir>/.env\n",
    "from mlfs import config\n",
    "#settings = config.HopsworksSettings(_env_file=f\"{root_dir}/.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e46aad",
   "metadata": {},
   "source": [
    "<span style=\"font-width:bold; font-size: 3rem; color:#333;\">- Part 02: Daily Feature Pipeline for Air Quality (aqicn.org) and weather (openmeteo)</span>\n",
    "\n",
    "## üóíÔ∏è This notebook is divided into the following sections:\n",
    "1. Download and Parse Data\n",
    "2. Feature Group Insertion\n",
    "\n",
    "\n",
    "__This notebook should be scheduled to run daily__\n",
    "\n",
    "In the book, we use a GitHub Action stored here:\n",
    "[.github/workflows/air-quality-daily.yml](https://github.com/featurestorebook/mlfs-book/blob/main/.github/workflows/air-quality-daily.yml)\n",
    "\n",
    "However, you are free to use any Python Orchestration tool to schedule this program to run daily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe638c6",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7de2e93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import hopsworks\n",
    "from mlfs.airquality import util\n",
    "from mlfs import config\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6081d1",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üåç Get the Sensor URL, Country, City, Street names from Hopsworks </span>\n",
    "\n",
    "__Update the values in the cell below.__\n",
    "\n",
    "__These should be the same values as in notebook 1 - the feature backfill notebook__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b70cd57d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-13 09:29:34,601 INFO: Closing external client and cleaning up certificates.\n",
      "Connection closed.\n",
      "2025-11-13 09:29:34,633 INFO: Initializing external client\n",
      "2025-11-13 09:29:34,634 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-13 09:29:35,944 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1272015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"country\": \"sweden\", \"city\": \"solna\", \"street\": \"anders-lundstroms-gata\", \"aqicn_url\": \"https://api.waqi.info/feed/A61420\", \"latitude\": \"59.36004\", \"longitude\": \"18.00086\"}'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = hopsworks.login(engine=\"python\")\n",
    "fs = project.get_feature_store() \n",
    "secrets = hopsworks.get_secrets_api()\n",
    "\n",
    "# This line will fail if you have not registered the AQICN_API_KEY as a secret in Hopsworks\n",
    "AQICN_API_KEY = secrets.get_secret(\"AQICN_API_KEY\").value\n",
    "location_str = secrets.get_secret(\"SENSOR_LOCATION_JSON\").value\n",
    "location = json.loads(location_str)\n",
    "\n",
    "country=location['country']\n",
    "city=location['city']\n",
    "street=location['street']\n",
    "aqicn_url=location['aqicn_url']\n",
    "latitude=location['latitude']\n",
    "longitude=location['longitude']\n",
    "\n",
    "today = datetime.date.today()- datetime.timedelta(days=5)\n",
    "\n",
    "location_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf9289",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\"> üîÆ Get references to the Feature Groups </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66f5d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve feature groups\n",
    "\n",
    "#For original\n",
    "air_quality_fg = fs.get_feature_group( \n",
    "    name='air_quality',\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "#For grade C\n",
    "air_quality_fg_with_lagged = fs.get_feature_group( \n",
    "    name='air_quality_including_lagged',\n",
    "    version=1,\n",
    ")\n",
    "\n",
    "weather_fg = fs.get_feature_group(\n",
    "    name='weather',\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b6ce8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7ffa41",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üå´ Retrieve Today's Air Quality data (PM2.5) from the AQI API</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6f681af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pm25</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>street</th>\n",
       "      <th>date</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>sweden</td>\n",
       "      <td>solna</td>\n",
       "      <td>anders-lundstroms-gata</td>\n",
       "      <td>2025-11-08</td>\n",
       "      <td>https://api.waqi.info/feed/A61420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pm25 country   city                  street       date  \\\n",
       "0   4.0  sweden  solna  anders-lundstroms-gata 2025-11-08   \n",
       "\n",
       "                                 url  \n",
       "0  https://api.waqi.info/feed/A61420  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "#For original\n",
    "aq_today_df = util.get_pm25(aqicn_url, country, city, street, today, AQICN_API_KEY)\n",
    "aq_today_df\n",
    "\n",
    "#For C-level\n",
    "aq__with_lagg_today_df = util.get_pm25(aqicn_url, country, city, street, today, AQICN_API_KEY)\n",
    "aq__with_lagg_today_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b9e24eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1 entries, 0 to 0\n",
      "Data columns (total 6 columns):\n",
      " #   Column   Non-Null Count  Dtype         \n",
      "---  ------   --------------  -----         \n",
      " 0   pm25     1 non-null      float32       \n",
      " 1   country  1 non-null      object        \n",
      " 2   city     1 non-null      object        \n",
      " 3   street   1 non-null      object        \n",
      " 4   date     1 non-null      datetime64[ns]\n",
      " 5   url      1 non-null      object        \n",
      "dtypes: datetime64[ns](1), float32(1), object(4)\n",
      "memory usage: 172.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "aq_today_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e83c0",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üëÄ Assignment for level C: Adding lagged air quality data </span>\n",
    "\n",
    "Adding the pm25 value for previous day, two days ago and three days ago as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bf560194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "2025-11-13 10:19:52,174 ERROR: [Errno 2] Opening HDFS file '/apps/hive/warehouse/id2223lab1isagro_featurestore.db/air_quality_including_lagged_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/id2223lab1isagro_featurestore.db/air_quality_including_lagged_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/id2223lab1isagro_featurestore.db/air_quality_including_lagged_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/id2223lab1isagro_featurestore.db/air_quality_including_lagged_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/id2223lab1isagro_featurestore.db/air_quality_including_lagged_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/id2223lab1isagro_featurestore.db/air_quality_including_lagged_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2025-11-13T10:19:52.171147+01:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/isabell/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py\", line 395, in afs_error_handler_wrapper\n",
      "    return func(instance, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/isabell/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py\", line 460, in read_query\n",
      "    return self._get_dataset(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/isabell/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/retrying.py\", line 55, in wrapped_f\n",
      "    return Retrying(*dargs, **dkw).call(f, *args, **kw)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/isabell/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/retrying.py\", line 279, in call\n",
      "    return attempt.get(self._wrap_exception)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/isabell/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/retrying.py\", line 326, in get\n",
      "    raise exc.with_traceback(tb)\n",
      "  File \"/Users/isabell/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/retrying.py\", line 273, in call\n",
      "    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)\n",
      "                      ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/isabell/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py\", line 446, in _get_dataset\n",
      "    reader = self._connection.do_get(info.endpoints[0].ticket, options)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_flight.pyx\", line 1745, in pyarrow._flight.FlightClient.do_get\n",
      "  File \"pyarrow/_flight.pyx\", line 58, in pyarrow._flight.check_flight_status\n",
      "pyarrow._flight.FlightServerError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/id2223lab1isagro_featurestore.db/air_quality_including_lagged_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n",
      "  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n",
      "    result = func(instance, *args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 196, in do_get\n",
      "    return self._read_query(context, path, command)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n",
      "    return func(instance, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n",
      "    result = func(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n",
      "    result_batches = self.hudi_query_engine.read_query(query_obj)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n",
      "    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n",
      "    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n",
      "    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n",
      "    self.open(\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n",
      "    f = self._open(\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n",
      "    stream = method(path, **_kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "FileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/id2223lab1isagro_featurestore.db/air_quality_including_lagged_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n",
      "  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n",
      "    raise FlyingDuckException(str(e)) from e\n",
      "utils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/id2223lab1isagro_featurestore.db/air_quality_including_lagged_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n",
      ". gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/id2223lab1isagro_featurestore.db/air_quality_including_lagged_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/id2223lab1isagro_featurestore.db/air_quality_including_lagged_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/id2223lab1isagro_featurestore.db/air_quality_including_lagged_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2025-11-13T10:19:52.171147+01:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal\n",
      "Error: Reading data from Hopsworks, using Hopsworks Feature Query Service           \n"
     ]
    },
    {
     "ename": "FeatureStoreException",
     "evalue": "Could not read data using Hopsworks Query Service.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFlightServerError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py:395\u001b[39m, in \u001b[36mArrowFlightClient._handle_afs_exception.<locals>.decorator.<locals>.afs_error_handler_wrapper\u001b[39m\u001b[34m(instance, *args, **kw)\u001b[39m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py:460\u001b[39m, in \u001b[36mArrowFlightClient.read_query\u001b[39m\u001b[34m(self, query_object, arrow_flight_config, dataframe_type)\u001b[39m\n\u001b[32m    459\u001b[39m descriptor = pyarrow.flight.FlightDescriptor.for_command(query_encoded)\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescriptor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43marrow_flight_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtimeout\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrow_flight_config\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/retrying.py:55\u001b[39m, in \u001b[36mretry.<locals>.wrap.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped_f\u001b[39m(*args, **kw):\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRetrying\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdkw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/retrying.py:279\u001b[39m, in \u001b[36mRetrying.call\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.should_reject(attempt):\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mattempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wrap_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.warning(attempt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/retrying.py:326\u001b[39m, in \u001b[36mAttempt.get\u001b[39m\u001b[34m(self, wrap_exception)\u001b[39m\n\u001b[32m    325\u001b[39m         exc_type, exc, tb = \u001b[38;5;28mself\u001b[39m.value\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exc.with_traceback(tb)\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/retrying.py:273\u001b[39m, in \u001b[36mRetrying.call\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m     attempt = Attempt(\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m, attempt_number, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py:446\u001b[39m, in \u001b[36mArrowFlightClient._get_dataset\u001b[39m\u001b[34m(self, descriptor, timeout, dataframe_type)\u001b[39m\n\u001b[32m    443\u001b[39m options = pyarrow.flight.FlightCallOptions(\n\u001b[32m    444\u001b[39m     timeout=timeout, headers=\u001b[38;5;28mself\u001b[39m._certificates_headers()\n\u001b[32m    445\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m reader = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_get\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mendpoints\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mticket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m _logger.debug(\u001b[33m\"\u001b[39m\u001b[33mDataset fetched. Converting to dataframe \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, dataframe_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/pyarrow/_flight.pyx:1745\u001b[39m, in \u001b[36mpyarrow._flight.FlightClient.do_get\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/pyarrow/_flight.pyx:58\u001b[39m, in \u001b[36mpyarrow._flight.check_flight_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mFlightServerError\u001b[39m: [Errno 2] Opening HDFS file '/apps/hive/warehouse/id2223lab1isagro_featurestore.db/air_quality_including_lagged_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\n  File \"/usr/src/app/src/server.py\", line 142, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 166, in wrapper\n    result = func(instance, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 196, in do_get\n    return self._read_query(context, path, command)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 123, in wrapper\n    return func(instance, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 131, in wrapper\n    result = func(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/server.py\", line 227, in _read_query\n    result_batches = self.hudi_query_engine.read_query(query_obj)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/hudi_query_engine.py\", line 63, in read_query\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 55, in get_featuregroup_parquet_paths\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/src/app/src/hudi_hopsfs_client.py\", line 185, in _get_partition_keys_from_metadata\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \"rt\") as f:\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1139, in open\n    self.open(\n  File \"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\", line 1151, in open\n    f = self._open(\n        ^^^^^^^^^^^\n  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 22, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 178, in _open\n    stream = method(path, **_kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/_fs.pyx\", line 789, in pyarrow._fs.FileSystem.open_input_file\n  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\nFileNotFoundError: [Errno 2] Opening HDFS file '/apps/hive/warehouse/id2223lab1isagro_featurestore.db/air_quality_including_lagged_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"pyarrow/_flight.pyx\", line 2255, in pyarrow._flight._do_get\n  File \"/usr/src/app/src/server.py\", line 145, in wrapper\n    raise FlyingDuckException(str(e)) from e\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file '/apps/hive/warehouse/id2223lab1isagro_featurestore.db/air_quality_including_lagged_1/.hoodie/hoodie.properties' failed. Detail: [errno 2] No such file or directory\n. gRPC client debug context: UNKNOWN:Error received from peer ipv4:51.79.26.27:5005 {grpc_message:\"[Errno 2] Opening HDFS file \\'/apps/hive/warehouse/id2223lab1isagro_featurestore.db/air_quality_including_lagged_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory. Detail: Python exception: Traceback (most recent call last):\\n  File \\\"/usr/src/app/src/server.py\\\", line 142, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 166, in wrapper\\n    result = func(instance, *args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 196, in do_get\\n    return self._read_query(context, path, command)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 123, in wrapper\\n    return func(instance, *args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 131, in wrapper\\n    result = func(*args, **kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/server.py\\\", line 227, in _read_query\\n    result_batches = self.hudi_query_engine.read_query(query_obj)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_query_engine.py\\\", line 63, in read_query\\n    hudi_featuregroups_paths[full_featuregroup_name] = self.hudi_hops_client.get_featuregroup_parquet_paths(\\n                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 55, in get_featuregroup_parquet_paths\\n    partition_keys = self._get_partition_keys_from_metadata(featuregroup_absolute_path)\\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/usr/src/app/src/hudi_hopsfs_client.py\\\", line 185, in _get_partition_keys_from_metadata\\n    with self.hopsfs.open(featuregroup_hoodie_properties_path.as_posix(), \\\"rt\\\") as f:\\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1139, in open\\n    self.open(\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/spec.py\\\", line 1151, in open\\n    f = self._open(\\n        ^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 22, in wrapper\\n    return func(*args, **kwargs)\\n           ^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"/opt/venv/lib/python3.11/site-packages/fsspec/implementations/arrow.py\\\", line 178, in _open\\n    stream = method(path, **_kwargs)\\n             ^^^^^^^^^^^^^^^^^^^^^^^\\n  File \\\"pyarrow/_fs.pyx\\\", line 789, in pyarrow._fs.FileSystem.open_input_file\\n  File \\\"pyarrow/error.pxi\\\", line 155, in pyarrow.lib.pyarrow_internal_check_status\\n  File \\\"pyarrow/error.pxi\\\", line 92, in pyarrow.lib.check_status\\nFileNotFoundError: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/id2223lab1isagro_featurestore.db/air_quality_including_lagged_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"pyarrow/_flight.pyx\\\", line 2255, in pyarrow._flight._do_get\\n  File \\\"/usr/src/app/src/server.py\\\", line 145, in wrapper\\n    raise FlyingDuckException(str(e)) from e\\nutils.exceptions.FlyingDuckException: [Errno 2] Opening HDFS file \\'/apps/hive/warehouse/id2223lab1isagro_featurestore.db/air_quality_including_lagged_1/.hoodie/hoodie.properties\\' failed. Detail: [errno 2] No such file or directory\\n\", grpc_status:2, created_time:\"2025-11-13T10:19:52.171147+01:00\"}. Client context: IOError: Server never sent a data message. Detail: Internal",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mFeatureStoreException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m query = query.filter(air_quality_fg_with_lagged.date == yesterday)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Materialize the query into a DataFrame\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df_yesterday = \u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#Now use yesterdays values and lagged values to update today's lagged values\u001b[39;00m\n\u001b[32m     15\u001b[39m aq__with_lagg_today_df[\u001b[33m\"\u001b[39m\u001b[33mlagged_aq_1_day\u001b[39m\u001b[33m\"\u001b[39m] = yesterday_df[\u001b[33m\"\u001b[39m\u001b[33mpm25\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/hsfs/constructor/query.py:309\u001b[39m, in \u001b[36mQuery.read\u001b[39m\u001b[34m(self, online, dataframe_type, read_options)\u001b[39m\n\u001b[32m    304\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.joins) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [f.type \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m schema]:\n\u001b[32m    305\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    306\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPandas types casting only supported for feature_group.read()/query.select_all()\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    307\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m    \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_feature_store_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43monline_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mread_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/hsfs/engine/python.py:146\u001b[39m, in \u001b[36mEngine.sql\u001b[39m\u001b[34m(self, sql_query, feature_store, online_conn, dataframe_type, read_options, schema)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msql\u001b[39m(\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    138\u001b[39m     sql_query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m     schema: Optional[List[feature.Feature]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    144\u001b[39m ) -> Union[pd.DataFrame, pl.DataFrame]:\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m online_conn:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sql_offline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m            \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m            \u001b[49m\u001b[43marrow_flight_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marrow_flight_config\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mread_options\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jdbc(\n\u001b[32m    156\u001b[39m             sql_query, online_conn, dataframe_type, read_options, schema\n\u001b[32m    157\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/hsfs/engine/python.py:189\u001b[39m, in \u001b[36mEngine._sql_offline\u001b[39m\u001b[34m(self, sql_query, dataframe_type, schema, arrow_flight_config)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sql_query, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mquery_string\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m sql_query:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhsfs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m arrow_flight_client\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     result_df = \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_with_loading_animation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mReading data from Hopsworks, using Hopsworks Feature Query Service\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43marrow_flight_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43marrow_flight_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataframe_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    197\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    198\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReading data with Hive is not supported when using hopsworks client version >= 4.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/hopsworks_common/util.py:373\u001b[39m, in \u001b[36mrun_with_loading_animation\u001b[39m\u001b[34m(message, func, *args, **kwargs)\u001b[39m\n\u001b[32m    370\u001b[39m end = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    374\u001b[39m     end = time.time()\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/KTH/KTH femman/Scalable Machine learning/id2223-lab1/.venv/lib/python3.11/site-packages/hsfs/core/arrow_flight_client.py:413\u001b[39m, in \u001b[36mArrowFlightClient._handle_afs_exception.<locals>.decorator.<locals>.afs_error_handler_wrapper\u001b[39m\u001b[34m(instance, *args, **kw)\u001b[39m\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(\u001b[38;5;28mstr\u001b[39m(e).split(\u001b[33m\"\u001b[39m\u001b[33mDetails:\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FeatureStoreException(user_message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mFeatureStoreException\u001b[39m: Could not read data using Hopsworks Query Service."
     ]
    }
   ],
   "source": [
    "#Reading yesterday's features and loads into pandas\n",
    "# Build a query that selects only pm25\n",
    "commits = air_quality_fg_with_lagged.commit_details()\n",
    "print(commits)\n",
    "query = air_quality_fg_with_lagged.select([\"pm25\"])\n",
    "\n",
    "# Add a filter on the date column\n",
    "yesterday = (today - datetime.timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "query = query.filter(air_quality_fg_with_lagged.date == yesterday)\n",
    "\n",
    "# Materialize the query into a DataFrame\n",
    "df_yesterday = query.read()\n",
    "\n",
    "#Now use yesterdays values and lagged values to update today's lagged values\n",
    "aq__with_lagg_today_df[\"lagged_aq_1_day\"] = yesterday_df[\"pm25\"]\n",
    "aq__with_lagg_today_df[\"lagged_aq_2_days\"] = two_days_ago_df[\"pm25\"]\n",
    "aq__with_lagg_today_df[\"lagged_aq_3_days\"] = three_days_ago_df[\"pm25\"]\n",
    "\n",
    "aq__with_lagg_today_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af845ab6",
   "metadata": {},
   "source": [
    "## <span style='color:#ff5f27'> üå¶ Get Weather Forecast data</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ecb3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates 59.25¬∞N 18.0¬∞E\n",
      "Elevation 13.0 m asl\n",
      "Timezone None None\n",
      "Timezone difference to GMT+0 0 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>temperature_2m_mean</th>\n",
       "      <th>precipitation_sum</th>\n",
       "      <th>wind_speed_10m_max</th>\n",
       "      <th>wind_direction_10m_dominant</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-11-13</td>\n",
       "      <td>9.60</td>\n",
       "      <td>0.2</td>\n",
       "      <td>15.542662</td>\n",
       "      <td>256.607483</td>\n",
       "      <td>solna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-11-14</td>\n",
       "      <td>3.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.275530</td>\n",
       "      <td>286.699310</td>\n",
       "      <td>solna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-11-15</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.720751</td>\n",
       "      <td>259.380402</td>\n",
       "      <td>solna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-11-16</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.440979</td>\n",
       "      <td>245.854462</td>\n",
       "      <td>solna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-11-17</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.792466</td>\n",
       "      <td>302.005341</td>\n",
       "      <td>solna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-11-18</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.308831</td>\n",
       "      <td>282.094727</td>\n",
       "      <td>solna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2025-11-19</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.4</td>\n",
       "      <td>12.245293</td>\n",
       "      <td>65.695465</td>\n",
       "      <td>solna</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  temperature_2m_mean  precipitation_sum  wind_speed_10m_max  \\\n",
       "0 2025-11-13                 9.60                0.2           15.542662   \n",
       "1 2025-11-14                 3.05                0.0           11.275530   \n",
       "2 2025-11-15                 1.50                0.0           11.720751   \n",
       "3 2025-11-16                 3.25                0.0           11.440979   \n",
       "4 2025-11-17                 0.50                0.0            6.792466   \n",
       "5 2025-11-18                 0.10                0.0           10.308831   \n",
       "6 2025-11-19                 0.90                0.4           12.245293   \n",
       "\n",
       "   wind_direction_10m_dominant   city  \n",
       "0                   256.607483  solna  \n",
       "1                   286.699310  solna  \n",
       "2                   259.380402  solna  \n",
       "3                   245.854462  solna  \n",
       "4                   302.005341  solna  \n",
       "5                   282.094727  solna  \n",
       "6                    65.695465  solna  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_df = util.get_hourly_weather_forecast(city, latitude, longitude)\n",
    "hourly_df = hourly_df.set_index('date')\n",
    "\n",
    "# We will only make 1 daily prediction, so we will replace the hourly forecasts with a single daily forecast\n",
    "# We only want the daily weather data, so only get weather at 12:00\n",
    "daily_df = hourly_df.between_time('11:59', '12:01')\n",
    "daily_df = daily_df.reset_index()\n",
    "daily_df['date'] = pd.to_datetime(daily_df['date']).dt.date\n",
    "daily_df['date'] = pd.to_datetime(daily_df['date'])\n",
    "daily_df['city'] = city\n",
    "daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c563109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7 entries, 0 to 6\n",
      "Data columns (total 6 columns):\n",
      " #   Column                       Non-Null Count  Dtype         \n",
      "---  ------                       --------------  -----         \n",
      " 0   date                         7 non-null      datetime64[ns]\n",
      " 1   temperature_2m_mean          7 non-null      float32       \n",
      " 2   precipitation_sum            7 non-null      float32       \n",
      " 3   wind_speed_10m_max           7 non-null      float32       \n",
      " 4   wind_direction_10m_dominant  7 non-null      float32       \n",
      " 5   city                         7 non-null      object        \n",
      "dtypes: datetime64[ns](1), float32(4), object(1)\n",
      "memory usage: 352.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "daily_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f5008",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:#ff5f27;\">‚¨ÜÔ∏è Uploading new data to the Feature Store</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9de5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-13 09:14:19,753 INFO: \t1 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1272015/fs/1258614/fg/1668524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1/1 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: air_quality_1_offline_fg_materialization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%6|1763021670.323|FAIL|rdkafka#producer-9| [thrd:ssl://51.161.80.189:9093/bootstrap]: ssl://51.161.80.189:9093/0: Disconnected (after 50180ms in state UP, 1 identical error(s) suppressed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1272015/jobs/named/air_quality_1_offline_fg_materialization/executions\n",
      "2025-11-13 09:14:34,096 INFO: \t1 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1272015/fs/1258614/fg/1703336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 1/1 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: air_quality_including_lagged_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1272015/jobs/named/air_quality_including_lagged_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('air_quality_including_lagged_1_offline_fg_materialization', 'SPARK'),\n",
       " {\n",
       "   \"success\": true,\n",
       "   \"results\": [\n",
       "     {\n",
       "       \"success\": true,\n",
       "       \"expectation_config\": {\n",
       "         \"expectation_type\": \"expect_column_min_to_be_between\",\n",
       "         \"kwargs\": {\n",
       "           \"column\": \"pm25\",\n",
       "           \"min_value\": -0.1,\n",
       "           \"max_value\": 500.0,\n",
       "           \"strict_min\": true\n",
       "         },\n",
       "         \"meta\": {\n",
       "           \"expectationId\": 747587\n",
       "         }\n",
       "       },\n",
       "       \"result\": {\n",
       "         \"observed_value\": 4.0,\n",
       "         \"element_count\": 1,\n",
       "         \"missing_count\": null,\n",
       "         \"missing_percent\": null\n",
       "       },\n",
       "       \"meta\": {\n",
       "         \"ingestionResult\": \"INGESTED\",\n",
       "         \"validationTime\": \"2025-11-13T08:14:34.000095Z\"\n",
       "       },\n",
       "       \"exception_info\": {\n",
       "         \"raised_exception\": false,\n",
       "         \"exception_message\": null,\n",
       "         \"exception_traceback\": null\n",
       "       }\n",
       "     }\n",
       "   ],\n",
       "   \"evaluation_parameters\": {},\n",
       "   \"statistics\": {\n",
       "     \"evaluated_expectations\": 1,\n",
       "     \"successful_expectations\": 1,\n",
       "     \"unsuccessful_expectations\": 0,\n",
       "     \"success_percent\": 100.0\n",
       "   },\n",
       "   \"meta\": {\n",
       "     \"great_expectations_version\": \"0.18.12\",\n",
       "     \"expectation_suite_name\": \"aq_expectation_suite\",\n",
       "     \"run_id\": {\n",
       "       \"run_name\": null,\n",
       "       \"run_time\": \"2025-11-13T09:14:34.096115+01:00\"\n",
       "     },\n",
       "     \"batch_kwargs\": {\n",
       "       \"ge_batch_id\": \"c9a958e6-c068-11f0-9304-62855a75a1e9\"\n",
       "     },\n",
       "     \"batch_markers\": {},\n",
       "     \"batch_parameters\": {},\n",
       "     \"validation_time\": \"20251113T081434.095753Z\",\n",
       "     \"expectation_suite_meta\": {\n",
       "       \"great_expectations_version\": \"0.18.12\"\n",
       "     }\n",
       "   }\n",
       " })"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert new data\n",
    "air_quality_fg.insert(aq_today_df)\n",
    "\n",
    "#For C-level\n",
    "air_quality_fg_with_lagged.insert(aq__with_lagg_today_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d491b0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-13 09:14:48,274 INFO: \t2 expectation(s) included in expectation_suite.\n",
      "Validation succeeded.\n",
      "Validation Report saved successfully, explore a summary at https://c.app.hopsworks.ai:443/p/1272015/fs/1258614/fg/1668525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%6|1763021689.975|FAIL|rdkafka#consumer-10| [thrd:GroupCoordinator]: GroupCoordinator: 51.161.81.208:9093: Disconnected (after 50116ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763021689.975|FAIL|rdkafka#consumer-10| [thrd:ssl://51.161.80.189:9093/bootstrap]: ssl://51.161.80.189:9093/0: Disconnected (after 50117ms in state UP, 1 identical error(s) suppressed)\n",
      "Uploading Dataframe: 100.00% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| Rows 7/7 | Elapsed Time: 00:01 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: weather_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai:443/p/1272015/jobs/named/weather_1_offline_fg_materialization/executions\n",
      "2025-11-13 09:15:05,525 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2025-11-13 09:15:08,753 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%6|1763021721.443|FAIL|rdkafka#producer-9| [thrd:ssl://51.161.80.189:9093/bootstrap]: ssl://51.161.80.189:9093/0: Disconnected (after 50128ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763021740.464|FAIL|rdkafka#consumer-10| [thrd:GroupCoordinator]: GroupCoordinator: 51.161.81.208:9093: Disconnected (after 50000ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763021740.705|FAIL|rdkafka#consumer-10| [thrd:ssl://51.161.80.189:9093/bootstrap]: ssl://51.161.80.189:9093/0: Disconnected (after 50241ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763021791.053|FAIL|rdkafka#consumer-10| [thrd:GroupCoordinator]: GroupCoordinator: 51.161.81.208:9093: Disconnected (after 50131ms in state UP, 1 identical error(s) suppressed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-13 09:16:41,927 INFO: Waiting for execution to finish. Current state: SUCCEEDING. Final status: UNDEFINED\n",
      "2025-11-13 09:16:48,333 INFO: Waiting for execution to finish. Current state: AGGREGATING_LOGS. Final status: SUCCEEDED\n",
      "2025-11-13 09:16:48,497 INFO: Waiting for log aggregation to finish.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%6|1763021816.858|FAIL|rdkafka#producer-9| [thrd:ssl://51.161.80.189:9093/bootstrap]: ssl://51.161.80.189:9093/0: Disconnected (after 94377ms in state UP, 1 identical error(s) suppressed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-13 09:16:57,150 INFO: Execution finished successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('weather_1_offline_fg_materialization', 'SPARK'),\n",
       " {\n",
       "   \"success\": true,\n",
       "   \"results\": [\n",
       "     {\n",
       "       \"success\": true,\n",
       "       \"expectation_config\": {\n",
       "         \"expectation_type\": \"expect_column_min_to_be_between\",\n",
       "         \"kwargs\": {\n",
       "           \"column\": \"wind_speed_10m_max\",\n",
       "           \"min_value\": -0.1,\n",
       "           \"max_value\": 1000.0,\n",
       "           \"strict_min\": true\n",
       "         },\n",
       "         \"meta\": {\n",
       "           \"expectationId\": 739347\n",
       "         }\n",
       "       },\n",
       "       \"result\": {\n",
       "         \"observed_value\": 6.792466163635254,\n",
       "         \"element_count\": 7,\n",
       "         \"missing_count\": null,\n",
       "         \"missing_percent\": null\n",
       "       },\n",
       "       \"meta\": {\n",
       "         \"ingestionResult\": \"INGESTED\",\n",
       "         \"validationTime\": \"2025-11-13T08:14:48.000273Z\"\n",
       "       },\n",
       "       \"exception_info\": {\n",
       "         \"raised_exception\": false,\n",
       "         \"exception_message\": null,\n",
       "         \"exception_traceback\": null\n",
       "       }\n",
       "     },\n",
       "     {\n",
       "       \"success\": true,\n",
       "       \"expectation_config\": {\n",
       "         \"expectation_type\": \"expect_column_min_to_be_between\",\n",
       "         \"kwargs\": {\n",
       "           \"column\": \"precipitation_sum\",\n",
       "           \"min_value\": -0.1,\n",
       "           \"max_value\": 1000.0,\n",
       "           \"strict_min\": true\n",
       "         },\n",
       "         \"meta\": {\n",
       "           \"expectationId\": 739348\n",
       "         }\n",
       "       },\n",
       "       \"result\": {\n",
       "         \"observed_value\": 0.0,\n",
       "         \"element_count\": 7,\n",
       "         \"missing_count\": null,\n",
       "         \"missing_percent\": null\n",
       "       },\n",
       "       \"meta\": {\n",
       "         \"ingestionResult\": \"INGESTED\",\n",
       "         \"validationTime\": \"2025-11-13T08:14:48.000273Z\"\n",
       "       },\n",
       "       \"exception_info\": {\n",
       "         \"raised_exception\": false,\n",
       "         \"exception_message\": null,\n",
       "         \"exception_traceback\": null\n",
       "       }\n",
       "     }\n",
       "   ],\n",
       "   \"evaluation_parameters\": {},\n",
       "   \"statistics\": {\n",
       "     \"evaluated_expectations\": 2,\n",
       "     \"successful_expectations\": 2,\n",
       "     \"unsuccessful_expectations\": 0,\n",
       "     \"success_percent\": 100.0\n",
       "   },\n",
       "   \"meta\": {\n",
       "     \"great_expectations_version\": \"0.18.12\",\n",
       "     \"expectation_suite_name\": \"weather_expectation_suite\",\n",
       "     \"run_id\": {\n",
       "       \"run_name\": null,\n",
       "       \"run_time\": \"2025-11-13T09:14:48.273791+01:00\"\n",
       "     },\n",
       "     \"batch_kwargs\": {\n",
       "       \"ge_batch_id\": \"d21cae1a-c068-11f0-9304-62855a75a1e9\"\n",
       "     },\n",
       "     \"batch_markers\": {},\n",
       "     \"batch_parameters\": {},\n",
       "     \"validation_time\": \"20251113T081448.273502Z\",\n",
       "     \"expectation_suite_meta\": {\n",
       "       \"great_expectations_version\": \"0.18.12\"\n",
       "     }\n",
       "   }\n",
       " })"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%6|1763021841.177|FAIL|rdkafka#consumer-10| [thrd:ssl://51.161.81.208:9093/bootstrap]: ssl://51.161.81.208:9093/2: Disconnected (after 150415ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763021841.655|FAIL|rdkafka#consumer-10| [thrd:GroupCoordinator]: GroupCoordinator: 51.161.81.208:9093: Disconnected (after 50120ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763021867.983|FAIL|rdkafka#producer-9| [thrd:ssl://51.161.81.188:9093/bootstrap]: ssl://51.161.81.188:9093/1: Disconnected (after 50122ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763021892.329|FAIL|rdkafka#consumer-10| [thrd:GroupCoordinator]: GroupCoordinator: 51.161.81.208:9093: Disconnected (after 50210ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763021892.329|FAIL|rdkafka#consumer-10| [thrd:ssl://51.161.81.208:9093/bootstrap]: ssl://51.161.81.208:9093/2: Disconnected (after 50210ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763021892.400|FAIL|rdkafka#consumer-10| [thrd:ssl://51.161.80.189:9093/bootstrap]: ssl://51.161.80.189:9093/0: Disconnected (after 50115ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763021919.263|FAIL|rdkafka#producer-9| [thrd:ssl://51.161.81.188:9093/bootstrap]: ssl://51.161.81.188:9093/1: Disconnected (after 50232ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763021942.821|FAIL|rdkafka#consumer-10| [thrd:GroupCoordinator]: GroupCoordinator: 51.161.81.208:9093: Disconnected (after 50001ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763021970.360|FAIL|rdkafka#producer-9| [thrd:ssl://51.161.81.208:9093/bootstrap]: ssl://51.161.81.208:9093/2: Disconnected (after 50147ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763021992.992|FAIL|rdkafka#consumer-10| [thrd:ssl://51.161.81.208:9093/bootstrap]: ssl://51.161.81.208:9093/2: Disconnected (after 100201ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763021993.405|FAIL|rdkafka#consumer-10| [thrd:GroupCoordinator]: GroupCoordinator: 51.161.81.208:9093: Disconnected (after 50117ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763022021.561|FAIL|rdkafka#producer-9| [thrd:ssl://51.161.81.208:9093/bootstrap]: ssl://51.161.81.208:9093/2: Disconnected (after 50166ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763022043.959|FAIL|rdkafka#consumer-10| [thrd:ssl://51.161.81.188:9093/bootstrap]: ssl://51.161.81.188:9093/1: Disconnected (after 50053ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763022044.107|FAIL|rdkafka#consumer-10| [thrd:GroupCoordinator]: GroupCoordinator: 51.161.81.208:9093: Disconnected (after 50233ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763022094.736|FAIL|rdkafka#consumer-10| [thrd:GroupCoordinator]: GroupCoordinator: 51.161.81.208:9093: Disconnected (after 50121ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763022116.816|FAIL|rdkafka#producer-9| [thrd:ssl://51.161.81.208:9093/bootstrap]: ssl://51.161.81.208:9093/2: Disconnected (after 94213ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763022144.957|FAIL|rdkafka#consumer-10| [thrd:ssl://51.161.81.208:9093/bootstrap]: ssl://51.161.81.208:9093/2: Disconnected (after 150719ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763022145.310|FAIL|rdkafka#consumer-10| [thrd:GroupCoordinator]: GroupCoordinator: 51.161.81.208:9093: Disconnected (after 50114ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763022168.101|FAIL|rdkafka#producer-9| [thrd:ssl://51.161.80.189:9093/bootstrap]: ssl://51.161.80.189:9093/0: Disconnected (after 50174ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763022195.833|FAIL|rdkafka#consumer-10| [thrd:GroupCoordinator]: GroupCoordinator: 51.161.81.208:9093: Disconnected (after 50052ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763022219.198|FAIL|rdkafka#producer-9| [thrd:ssl://51.161.80.189:9093/bootstrap]: ssl://51.161.80.189:9093/0: Disconnected (after 50097ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763022246.009|FAIL|rdkafka#consumer-10| [thrd:ssl://51.161.80.189:9093/bootstrap]: ssl://51.161.80.189:9093/0: Disconnected (after 100233ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763022246.422|FAIL|rdkafka#consumer-10| [thrd:GroupCoordinator]: GroupCoordinator: 51.161.81.208:9093: Disconnected (after 50114ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763022270.395|FAIL|rdkafka#producer-9| [thrd:ssl://51.161.80.189:9093/bootstrap]: ssl://51.161.80.189:9093/0: Disconnected (after 50118ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763022296.898|FAIL|rdkafka#consumer-10| [thrd:GroupCoordinator]: GroupCoordinator: 51.161.81.208:9093: Disconnected (after 50002ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1763022321.785|FAIL|rdkafka#producer-9| [thrd:ssl://51.161.80.189:9093/bootstrap]: ssl://51.161.80.189:9093/0: Disconnected (after 50127ms in state UP, 1 identical error(s) suppressed)\n"
     ]
    }
   ],
   "source": [
    "# Insert new data\n",
    "weather_fg.insert(daily_df, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e9e2d",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">‚è≠Ô∏è **Next:** Part 03: Training Pipeline\n",
    " </span> \n",
    "\n",
    "In the following notebook you will read from a feature group and create training dataset within the feature store\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
